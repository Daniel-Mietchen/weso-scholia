# Meeting 08-July-2021

## Attendees 

- Daniel Mietchen
- Guillermo
- Pablo
- Jorge
- Labra
- Dani F.
- Egon

## Slides

* [Here](https://docs.google.com/presentation/d/1xp6eUqFEfONaaAB3G15ul2RkykQuE-mxYWPd05CMJx4/edit)

## Recording

- Daniel Mietchen tried to record the meeting but the recording stopped after 8 min. Anyway, the recorded file is available [on Youtube](https://youtu.be/8_j5dHeeuuM) (unlisted).

## Questions and feedback

- Daniel Mietchen, regarding Wikidata Federation:  "Once we have a process that works, we can request our endpoint to be whitelisted for federation from Wikidata's endpoint"
- Daniel Mietchen proposal on Pablo's tests: try to compute 2014 Wikidata dumps (much smaller) as a starting point for troublesome processes, e.g. to find out how consistent the ordering is in Blazegraph results using an OFFSET clause
- Daniel Mietchen on loosing some nodes with limit/offset strategy: it may be solved once we run the queries on our own server. Several Wikidata servers may be producing different sorting orders (e.g. see the ```Lag``` graph [here](https://grafana.wikimedia.org/d/000000489/wikidata-query-service?orgId=1&refresh=1m)). 
- Daniel Mietchen: Maybe we can decompose Scholia's SPARQL queries into parts that are standard SPARQL and those that are not. Switching to standard SPARQL queries would allow us to use HDT, LD Fragments, or some other technologies. Not changing the SPARQL queries was the reasons that prevented us to explore those alternatives. But we are indeed modifying the SPARQL queries.
- Egon: What about the problem of actually synchronizing Scholia and Wikidata if we use our local dump/endpoint, but the process to load it takes so much time?
    - Daniel Mietchen: Once there is a base dump loaded, we could continue working with incremental dumps.
- Daniel Mietchen: it may be aceptable to have a week-delay with Wikidata as a price to pay to increase performance.
- Daniel Mietchen on cache approaches: go for it. Intermediate results can work not just for performance stuff, but maybe also help to produce some other richer visualizations in scholia.
- Daniel Mietchen: the main goal for the summer would be to have a prototype working. It doesn't need to be working perfectly for every Scholia feature.
- Egon: example on how to maintain an updated subset of Wikidata elements: https://github.com/egonw/wikidata-chemistry 
- Daniel Mietchen: once we have a clear workflow, maybe we could ask Wikidata for dumps that fit better our workflow.
- Daniel Mietchen, on Wikidata's BNodes: Apparently, there is some recent work (maybe not yet implemented) about modifying how to represent BNodes in Wikidata. We should be aware fo that as it may affect our workflows.
- Daniel Mietchen: mind language information to produce subsets:
    - We may avoid any other language than English while processing Scholia queries internally.
    - We may get multilingually labelled strings later just for the final results of a Scholia query, perhaps directly from the actual Wikidata endpoint.
- Daniel Mietchen: specs to host a Wikidata endpoint are interesting. We are on discussions with Wikidata to get a dedicated server and production support.
    - Current estimated specs: 16 Cores, 100 Gb Memory, 1.125Gb Disk and Ethernet.

